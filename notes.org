; -*- mode: org;-*-

* decision tree

It is odd that pruning did not seem to make a difference in testing
accuracy for bottom-up, while in top-down there was more of an effect.
As expected, overfitting in top-down did not occur when the tree was
small (in nodes and in depth).



# top-down
expectations (top-down): Larger values of \epsilon, because it leads to
less pruning, will slow down the pruning of the tree.

observations:

The apparent "tipping" points in \epsilon depend on splitting allowed
by the information gain in the dataset.

If \epsilon is too large, no pruning occurs. If the training tree was
perfect on the validation data, no pruning would be needed, hence it
would take the longest.

It doesn't take too much time to do a prune that avoids overfitting
(\epsilon=0.006), due perhaps to the greedy procedure of using the
largest information gain, which places the most informative nodes at the
top.

That there are 5 "levels" where accuracy and tree size jumps, would that
mean 5 features contain most of the information? Or should it be the
depth of the tree (5 or 7) where overfitting does not occur?




# bottom-up
Unlike top-down pruning that can remove entire subtrees, bottom-up
cannot, so it will evaluate more nodes because it removes them one by
one. Smaller \epsilon values does lead to faster pruning times, because
more is pruned.

** pruning and timing
*** without pruning

data:
#+BEGIN_EXAMPLE
In [54]: _ca(mtx_cnt_train), _ca(mtx_cnt_test)
Out[54]: (1.0, 0.81461352657004826)

In [55]: tree_stats(tree)
Out[55]: (1153, 11)
#+END_EXAMPLE
overfitting

*** top down

data:
#+BEGIN_EXAMPLE
    e  cap_train  cap_test     n   d           t
0.001   0.894107  0.891189    19   5   18.285562
0.002   0.894107  0.891189    19   5   18.720711
0.003   0.895879  0.891189    51   7   23.813625
0.004   0.895879  0.891189    51   7   22.615348
0.005   0.895879  0.891189    51   7   23.128672
0.006   0.895879  0.891189    51   7   21.832072
0.007   0.909615  0.877370   224   8   170.941651
0.008   0.914931  0.871445   282   8   245.560105
0.009   0.914931  0.871445   282   8   309.202023
0.010   0.914931  0.871445   282   8   247.727893
0.011   0.914931  0.871445   282   8   338.332978
0.012   0.914931  0.871445   282   8   334.193085
0.013   0.914931  0.871445   282   8   235.429013
0.014   0.914931  0.871445   282   8   253.089367
0.015   0.914931  0.871445   282   8   228.933522
0.016   0.914931  0.871445   282   8   229.970629
0.020   0.972087  0.843468   837  11   2370.405390
0.024   0.972087  0.843468   837  11   2244.040782
0.028   1.000000  0.814614  1153  11   3653.713521
0.032   1.000000  0.814614  1153  11   3472.295189
0.064   1.000000  0.814614  1153  11   3492.725905
0.160   1.000000  0.814614  1153  11   3623.630783
#+END_EXAMPLE
t in seconds. time is for pruning and classification accuracy. runs
were in done in irregular batches.

*** bottom up

data:
#+BEGIN_EXAMPLE
$ make
python dtree.py --train data/hw1/noisy10_train.ssv --test data/hw1/noisy10_test.ssv --valid data/hw1/noisy10_valid.ssv
        e  cap_train  cap_test     n   d            t
0  0.0001   0.929553  0.881446  1153  11  3475.721535
1  0.0005   0.932654  0.880357  1153  11  3697.923060
2  0.0010   0.942844  0.868609  1153  11  3771.361709
3  0.0050   0.973859  0.832436  1153  11  4504.712725
4  0.0100   0.979176  0.831239  1153  11  4478.894439
5  0.0300   1.000000  0.814614  1153  11  4898.032025
#+END_EXAMPLE

** future

- prune the tree as it is being grown is faster?

** todo

- fix n and d counts in bottom-up pruning

* naive bayes
** hw qs

q3.1: What if $P(X_i|Y) =_{\text{maybe}} P(X_j|Y)$ rather than necessarily?
#
For a given label Y, then for a given word $X_i$, position 1 would need an
estimated parameter, as will position 2, 3, and so on. For the next
$X_{i+1}$ (in the known vocabulary), an entirely new set of estimates
would be required. This means $\abs{V}$ words times the number of word
positions times m. Removing the word positions simplifies the model for
tractability, so that one can count just the frequency of a word, per
category.

q3.2
#+BEGIN_EXAMPLE
accuracy: 0.7852098600932712

pred   1    2    3    4    5    6    7    8    9    10   11   12   13   14   15   16   17   18   19   20
base                                                                                                    
1     249    0    0    0    0    1    0    0    1    0    0    2    0    3    3   24    2    3    4   26
2       0  286   13   14    9   22    4    1    1    0    1   11    8    6   10    1    2    0    0    0
3       1   33  204   57   19   21    4    2    3    0    0   12    5   10    8    3    1    0    5    3
4       0   11   30  277   20    1   10    2    1    0    1    4   32    1    2    0    0    0    0    0
5       0   17   13   30  269    0   12    2    2    0    0    3   21    8    4    0    1    0    1    0
6       0   54   16    6    3  285    1    1    3    0    0    5    3    6    4    0    1    1    1    0
7       0    7    5   32   16    1  270   17    8    1    2    0    7    4    6    0    2    1    2    1
8       0    3    1    2    0    0   14  331   17    0    0    1   13    0    4    2    0    0    6    1
9       0    1    0    1    0    0    2   27  360    0    0    0    3    1    0    0    1    1    0    0
10      0    0    0    1    1    0    2    1    2  352   17    0    1    3    3    5    2    1    5    1
11      2    0    1    0    0    0    2    1    2    4  383    0    0    0    0    1    2    0    1    0
12      0    3    0    3    4    1    0    0    0    1    1  362    2    2    2    0    9    0    5    0
13      3   20    4   25    7    4    8   11    6    0    0   21  264    9    7    1    3    0    0    0
14      5    7    0    3    0    0    3    5    4    1    0    1    8  320    8    7    6    5    8    2
15      0    8    0    1    0    3    1    0    1    0    1    4    6    5  343    3    2    1   12    1
16     11    2    0    0    0    2    1    0    0    0    0    0    0    2    0  362    0    1    2   15
17      1    1    0    0    0    1    1    2    1    1    0    4    0    5    2    1  303    5   23   13
18     12    1    0    1    0    0    1    2    0    2    0    2    1    0    0    6    3  326   18    1
19      6    1    0    0    1    1    0    0    0    0    0    5    0   10    6    2   63    6  196   13
20     39    3    0    0    0    0    0    0    1    1    0    1    0    2    6   27   10    3    7  151
#+END_EXAMPLE

q3.3: The highest % confusion comes from =talk.*.misc= and =comp.*= (and
=sci.electronics=), and the least from =rec.*=
#
Under $\alpha = \tfrac{1}{\abs{V}}$, confusion occurs more often when the topic of
two documents is similar. This is likely due to similar word distributions,
especially of content words. An addition factor might be the
inter-related-ness of the underlying phenomenon, or the similar
interests of users.

q3.4: Why does test accuracy drop for small or large \alpha?
